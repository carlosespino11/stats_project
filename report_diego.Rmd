---
title: "Untitled"
output: pdf_document
---

# Linear analysis

```{R, results = 'hide', echo=FALSE}
library(ISLR)
library(leaps)
library(glmnet)
library(rpart.plot)
library(caret)
library(MASS)
library(GGally)
library(grid)
library(ggplot2)
library(gridExtra)
library(splines)
library(caret)
library(broom)
load("ggthemes_data.rda")

######################################################################
# Setup
######################################################################
source("theme_fivethirtyeight.R")
Crime <- read.csv("Crime.csv")
Crime = Crime[,-1] # Remove the index column

# Identify and remove the outliers found from later analyses
# fit.all <- lm(crmrte~bs(pctmin, knots=pctmin.knots) + log(prbconv) + log(polpc) + prbarr + density:county + prbarr:prbpris + pctmin:polpc + polpc:wfed + density:pctmin + density:pctymle + taxpc:wfed + region:wsta, data=Crime)
bad = c(440,353,437,439)
Crime = Crime[-bad,]
```

Additionally, we considered a standard _linear regression_ model involving all predictors, as an alternative means to view the significance of each predictor. Note that in this context, performing _k-fold cross-validation_ or _bootstrapping_ wouldn't allow us to come up with an "averaged" subset, hence we performed an ordinary $80/20$ splitting of the data into the testing an training as shown in the code below: 

```{R}
set.seed(1000)
test = sample(nrow(Crime), nrow(Crime)*.2)
excluded = c("crmrte", "crmrte_cat", "region_w_nw")
xs = Crime[-which(names(Crime) %in% excluded)]
ys = Crime[which(names(Crime) %in% c("crmrte"))]
xs_test = xs[test,]
xs_train = xs[-test,]
ys_test = ys[test,]
ys_train = ys[-test,]
p = dim(xs)[2]
```

We then run a simple linear fit will all predictors, in order to analyse the significance levels of the parameters, provided that the linear test itself has a significant $R^2$ value. 

```{R}
lm.fit = lm(crmrte ~ ., data = Crime)
summary(lm.fit)
```

as the $R^2$ value is sufficiently high (`r summary(lm.fit)$r.squared`), we decided to perform `best subset selection` on the set of predictors. Although we are aware of the performance penalties of doing this for $p = 23$, the running times were considerably short and hence we decided to stick to this approach: 

```{R, cache = TRUE}
bestsubset=regsubsets(y ~ ., data = data.frame(y = ys_train, x = xs_train), nvmax = p)
```

After getting all best subsets with size $k = 1...p$, we analysed both the _training RSE_ and _testing RSE_ by performing _k-fold cross validation_ with $k = 10$ and then getting the minimum errors on all iterations: 


```{R}
set.seed(1000)
x_cols = colnames(xs, do.NULL = FALSE, prefix = "x.")
colnames(xs) <- paste("x", x_cols, sep = ".")
x_cols = colnames(xs)
folds <- createFolds(Crime$crmrte, k=10, list=TRUE, returnTrain=FALSE)
val.test.errors = matrix(, nrow = length(folds), ncol = p)
val.train.errors = matrix(, nrow = length(folds), ncol = p)
pred_train = vector()
pred_test = vector()

for (j in 1:length(folds)) {
  test     = folds[[j]]
  ys_train = ys[-test,]
  ys_test  = ys[test,]
  xs_train = xs[-test,]
  xs_test  = xs[test,]
  for (i in 1:p) {
    coefi = coef(bestsubset, id = i)
    pred_train = as.matrix(xs_test[, x_cols %in% names(coefi)]) %*% coefi[names(coefi) 
                                                                    %in% x_cols]
    pred_test = as.matrix(xs_train[, x_cols %in% names(coefi)]) %*% coefi[names(coefi) 
                                                                     %in% x_cols]
    val.train.errors[j,i] = mean((ys_train - pred_test)^2)
    val.test.errors[j,i] = mean((ys_test - pred_train)^2)
  }
}
min_test = which(val.test.errors == min(val.test.errors), arr.ind = TRUE) 
min_test
min_train = which(val.train.errors == min(val.train.errors), arr.ind = TRUE) 
min_train
min_test_error.1 = val.test.errors[min_test]
min_test_error.1
min_train_error.1 = val.train.errors[min_train]
min_train_error.1
```

The ratio between _testing RSE_ and _training RSE_ is very close to $1$ (`r min_test_error.1 / min_train_error.1`) for the subset that minimizes both _RSEs_  (which has `r min_test[,2]` predictors). Consequently, we can conclude that the predictors yielded by the subset generated by _best subset selection_ which minimizes both _RSEs_ belong to a consistent model and, hence, can be used as a basis for non linear models. Nevertheless, we decided to run a linear fit with these predictors in order to check our conclusions: 

```{R}
lm.2.fit = lm(crmrte ~  prbarr + prbconv + polpc + density 
              + as.factor(region) + pctmin + wfed + pctymle, data = Crime)
summary(lm.2.fit)
```

We can note that all coefficients are significant and the $R^2$, as expected, was reduced but only marginally (`r summary(lm.2.fit)$r.squared` versus `r summary(lm.fit)$r.squared`), which confirms that the model with this subset is indeed a good model. 

Next, we proceeded to graphically analyse any nonlinearities between these predictors and the response, by looking at all pairwise plots: 

```{R}
fitnames = c("prbarr" , "prbconv" , "polpc" , "density" ,
             "as.factor(region)" , "pctmin" , "wfed" ,
             "pctymle", "crmrte")
pairs(Crime[names(Crime) %in% fitnames])
```

It can be seen that `prbrarr`, `prbconv`and `polpc` have a peak structure that would benefit from applying a _log_ to them in order to shrink those peaks. Additionally, `wfed` has a nonlinear relationship with `wfed`, which makes it suitable as a polynomial regression predictor. Consequently, we run a new, nonlinear model with these modified predictors: 

```{R}
lm.3.fit = lm(crmrte ~  log(prbarr) + log(prbconv) +
                log(polpc) + density + as.factor(region) + 
                pctmin + poly(wfed,3) + pctymle, data = Crime)
summary(lm.3.fit)
```

The lack of significance of the polynomials for `wfed` and the increase in $R^2$ suggests that this model _overfits_. Hence, we decided to remove the polynomials related to `wfed`, but kept the $log$ predictors as they have shown to be still very significant.

Next, we analysed all significant interactions between all interaction terms:

```{R}
lm.4.fit = lm(crmrte ~  .*., data = Crime)
summary(lm.4.fit)$r.squared
```

and plugged all interactions to out previous model: 

```{R}
lm.5.fit = lm(crmrte ~  log(prbarr) + log(prbconv) +
                log(polpc) + density + as.factor(region) + 
                pctmin + poly(wfed,3) + pctymle + .*., data = Crime)
summary(lm.5.fit)$r.squared
```

The number of interactions that we added to the model are: 

```{R}
dim(summary(lm.4.fit)$coefficients)
```

We can see that both models are seriously overfitting ($R^2$ values are `r summary(lm.4.fit)$r.squared` and `r summary(lm.5.fit)$r.squared` respectively, while the number of predictors has skyrocketed due to all interaction combinations). Even though it is tempting to keep only those interactions with a relevant significance value, since the removal of each of these predictors affects the overall model, we decided instead to choose a final model by using a _Stepwise Algorithm_ applying _AIC_ to decide. The stepwise procedure is too lengthy and cumbersome to show in text, but the code to generated is displayed below: 

```{R, results='hide', cache = TRUE}
interaction.fit = stepAIC(lm.5.fit)
```

Once we got the fit, which has the following number of coefficients: 

```{R}
length(interaction.fit$coefficients)
```

which means a reduction on the number of interactions by $30\%$, we proceeded to calculate both _training MSE_ and _testing MSE_: 


```{R}
coefi = coef(interaction.fit)
pred = as.matrix(xs_train[, x_cols %in% names(coefi)]) %*% coefi[names(coefi) 
                                                                %in% x_cols]
val.train.errors = mean((ys_train - pred)^2)
val.train.errors

pred = as.matrix(xs_test[, x_cols %in% names(coefi)]) %*% coefi[names(coefi) 
                                                                %in% x_cols]
val.test.errors = mean((ys_test - pred)^2)
val.test.errors
```

Here, we can see that the error rate remains close to one (`r val.test.errors / val.train.errors`) which still proves that this model holds. Now that we obtained a complex model consisting of linear variables, _log_ variables and _interaction_ variables, we're gonna perform _Lasso_ in order to remove all interaction terms that are not significant, so that we arrive to a model easy to understand.

# Lasso Analysis

With the resulting model from all our previous steps, we performed _k-fold cross validation_ using Lasso, in order to obtain the optimum value of $\lambda$ for our model. The code generating the lasso is the following (note that the first line is a way to manually represent `interaction.fit` as the fit is not recognized by `cv.glmnet`, which requires a formula with a specific formatting): 

```{R}
formula = "~ log(prbconv) + log(polpc) + density + pctmin +  poly(wfed, 3) +
pctymle + county + year + prbarr + prbconv +   prbpris + avgsen + polpc +
taxpc + region + smsa + wcon +   wtuc + wtrd + wfir + wser + wmfg + wfed + 
wsta + wloc + mix +   county:year + county:avgsen + county:polpc + 
density:county +  pctmin:county + county:wcon + county:wtuc + county:wtrd +
county:wfir + county:wmfg + county:wsta + county:wloc + pctymle:county +   
year:prbconv + year:prbpris + year:polpc + year:region +   year:smsa + pctmin:year + 
year:wtrd + year:wfir + year:wmfg +   year:wsta + year:mix + pctymle:year +
prbarr:prbpris + prbarr:polpc +   density:prbarr + prbarr:region + pctmin:prbarr + 
prbarr:wcon +   prbarr:wtuc + prbarr:wtrd + prbarr:wfir + prbarr:wfed +
prbconv:prbpris +   prbconv:polpc + prbconv:smsa + pctmin:prbconv + prbconv:wcon +   
prbconv:wfir + prbconv:wser + prbconv:wmfg + prbconv:mix +   density:prbpris +
prbpris:taxpc + prbpris:region + pctmin:prbpris +   prbpris:wcon + prbpris:wtrd + 
prbpris:wmfg + prbpris:wfed + prbpris:wsta + prbpris:wloc + density:avgsen + 
avgsen:taxpc + avgsen:region + avgsen:smsa + pctmin:avgsen + avgsen:wcon +   
avgsen:wtrd + avgsen:wfir + avgsen:wser + avgsen:wfed + avgsen:mix + pctymle:avgsen + 
polpc:region + polpc:smsa + pctmin:polpc +   polpc:wtuc + polpc:wtrd + polpc:wser + 
polpc:wmfg + polpc:wfed +density:region + density:smsa + density:pctmin + 
density:wcon +  density:wtuc + density:wtrd + density:wsta + density:mix +   
density:pctymle + taxpc:region + taxpc:smsa + pctmin:taxpc + 
taxpc:wmfg + taxpc:wfed + taxpc:wsta + taxpc:wloc + region:smsa +   
pctmin:region + region:wcon + region:wtuc + region:wtrd +   region:wfir + 
region:wser + region:wmfg + region:wfed + region:wsta +   region:wloc + 
region:mix + pctymle:region + pctmin:smsa +   smsa:wtuc + smsa:wtrd + 
smsa:wser + smsa:wmfg + smsa:wfed +   smsa:wsta + smsa:mix + pctymle:smsa + 
pctmin:wtrd + pctmin:wfir +   pctmin:wser + pctmin:wmfg + pctmin:wfed + 
pctmin:wsta + pctmin:wloc +   pctmin:mix + pctmin:pctymle + wcon:wtuc +
wcon:wfir + wcon:wmfg + wcon:wfed + wcon:wsta + wcon:mix + pctymle:wcon + 
wtuc:wsta +   wtuc:wloc + wtrd:wfir +wtrd:wmfg + wtrd:wsta + wtrd:mix + 
wfir:wmfg + wfir:wfed + wfir:wsta + wfir:wloc + wfir:mix + pctymle:wfir + 
wser:wmfg + wser:wfed + wser:wloc + wser:mix + wmfg:wfed + pctymle:wmfg + 
wfed:wsta + wfed:wloc + wfed:mix +   wsta:wloc + wsta:mix + pctymle:wsta 
+ pctymle:mix"
xs_lasso = model.matrix(as.formula(formula), Crime)
xs_lasso_train = xs_lasso[-test,]
xs_lasso_test = xs_lasso[test,]
grid=10^seq(2,-4,length=100)
lasso.mod=cv.glmnet(xs_lasso[-test ,],ys_train,alpha=1,lambda=grid)
```

Now that we have computed all possible values for $lambda$, we can create the plot showing the _training RSE_ and $\lambda$ increases: 

```{R}
ggplot.glmnet = function(cv.glmnet.obj){
  tidied_cv <- tidy(cv.glmnet.obj)
  glance_cv <- glance(cv.glmnet.obj)

  # plot of MSE as a function of lambda
  g <- ggplot(tidied_cv, aes(lambda, estimate))  + scale_x_log10() + 
    geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = .15)+ 
    geom_line(colour = "red") +
    # geom_line(aes(y=conf.low), linetype = "dashed",size=.4)+
    # geom_line(aes(y=conf.high), linetype = "dashed",size=.4)+
    geom_vline(xintercept = glance_cv$lambda.min, lty = 2) +
    geom_vline(xintercept = glance_cv$lambda.1se, lty = 2) + theme_fivethirtyeight()
  return(g)
} 
ggplot.glmnet(lasso.mod)
plot(lasso.mod)

```

We then chose a value of $\lambda$ within $1$ standard deviation from the optimum value, as this is a commonly established good practice. The fit with that value is performed in the code below: 

```{R}
lasso.mod.3=glmnet(xs_lasso[-test ,],ys_train,alpha=1,lambda=lasso.mod$lambda.1se)
lasso_vars =  names(coef(lasso.mod.3)[,1][coef(lasso.mod.3)[,1]!=0])[-1]
length(lasso_vars)
```

Lasso actually yielded `r length(lasso_vars)` variables with nonzero values, a reduction of `r round(((length(interaction.fit$coefficients) - length(lasso_vars)) / length(interaction.fit$coefficients))*100)`$\%$ with respect to the _stepwise AIC_. The _testing RSE_ for this model is obtained using the same _k-folds_ generated for the first model:  

```{R}
lasso_rse = vector()

for (i in 1:length(folds)) {
  test = folds[[i]]
  xs_lasso_train = xs_lasso[-test,]
  xs_lasso_test = xs_lasso[test,]
  ys_test = ys[test,]
  ys_lasso_pred = predict(lasso.mod.3, xs_lasso_test, s = lasso.mod$lambda.1se)
  lasso_rse[i] = mean((ys_test - ys_lasso_pred)^2)
}

mean(lasso_rse)
```

and the error rate w.r.t. to our original model using _best subset selection_ is: 

```{R}
(mean(lasso_rse) / min_test_error.1) * 100
```

Finally, we analyse the relative average $L1$ distance between our estimators and the true model: 

```{R}
mean(abs((ys_test - ys_lasso_pred)/ ys_test))
```

which yields a more than reasonable value, since it's below $30\%$. Consequently, the final set obtained in this section, after performing _linear_, _best subset selection_, _log_, _polynomial_ , _stepwise AIC_ and _Lasso_ yielded a model that will be used in the following sections for more complex fits that will derive in our final model.

